# Full Pipeline Audit — 2025-07-18

Comprehensive re-audit of the entire DTFHN codebase. Previous audit (`nohup-pipeline-audit.md`) found 3 issues (all fixed). This is a clean audit of every Python file in `src/` and `scripts/`, plus `run_episode.sh`.

## Files Audited

- `src/__init__.py`
- `src/hn.py`
- `src/scraper.py`
- `src/generator.py`
- `src/pipeline.py`
- `src/storage.py`
- `src/tts.py`
- `src/audio.py`
- `src/chapters.py`
- `src/metadata.py`
- `src/transcript.py`
- `src/embeddings.py`
- `scripts/run_episode.sh`
- `scripts/generate_episode_audio.py`
- `scripts/generate_missing_wavs.py`
- `scripts/scrape_and_load.py`
- `scripts/refetch_test.py`
- `requirements.txt`

---

## Issues Found

### ISSUE 1 — HIGH: `scripts/generate_missing_wavs.py` — Skips em-dash breathing pauses

**File:** `scripts/generate_missing_wavs.py`  
**Line:** 37–43  
**Severity:** HIGH  

**What breaks:** This script sends raw text directly to the TTS API (`requests.post(TTS_URL, json={"text": text, ...})`) without calling `prepare_text_for_tts()` from `src/tts.py`. The main TTS pipeline wraps text with em-dashes (`— text —`) to create natural breathing pauses. WAVs generated by this script will sound different — abrupt starts/stops vs. natural breathing room.

**Impact:** If this script regenerates segments for a partially-failed episode, those segments will have audibly different pacing than the rest of the episode.

**Fix:**
```python
# Add import at top
sys.path.insert(0, str(Path(__file__).parent.parent))
from src.tts import prepare_text_for_tts

# In the loop, wrap text before sending:
prepared = prepare_text_for_tts(text)
resp = requests.post(TTS_URL, json={"text": prepared, ...})
```

---

### ISSUE 2 — HIGH: `scripts/generate_missing_wavs.py` — Hardcoded episode date

**File:** `scripts/generate_missing_wavs.py`  
**Line:** 10  
**Severity:** HIGH  

**What breaks:** `EPISODE_DIR` is hardcoded to `data/episodes/2026-01-29`. This script is useless for any other episode.

**Fix:** Accept episode date as CLI argument:
```python
import argparse
parser = argparse.ArgumentParser()
parser.add_argument("episode_date", help="Episode date (YYYY-MM-DD or YYYY-MM-DD-HHMM)")
args = parser.parse_args()
EPISODE_DIR = Path(__file__).parent.parent / "data" / "episodes" / args.episode_date
```

---

### ISSUE 3 — HIGH: `scripts/generate_missing_wavs.py` — No WAV validation

**File:** `scripts/generate_missing_wavs.py`  
**Line:** 49–50  
**Severity:** HIGH  

**What breaks:** The script checks `resp.status_code == 200` but doesn't validate the WAV content (RIFF header, minimum size). A 200 response with garbage bytes would be written as a `.wav` file and considered "complete." The robust pipeline in `src/tts.py` has `validate_wav_bytes()` for this exact reason.

**Fix:** Use `validate_wav_bytes()` from `src/tts.py` before writing:
```python
from src.tts import validate_wav_bytes
# After resp.status_code == 200:
is_valid, error = validate_wav_bytes(resp.content)
if not is_valid:
    print(f"INVALID WAV: {error}")
    continue
```

---

### ISSUE 4 — MEDIUM: `scripts/scrape_and_load.py` — Uses `YYYY-MM-DD` format, not `YYYY-MM-DD-HHMM`

**File:** `scripts/scrape_and_load.py`  
**Line:** 147  
**Severity:** MEDIUM  

**What breaks:** `episode_date = date.today().isoformat()` produces `2026-01-29` (no time component). The pipeline standard is now `YYYY-MM-DD-HHMM`. Stories stored with this format won't match episodes generated by the main pipeline (which uses `YYYY-MM-DD-HHMM`), causing `get_stories_by_date()` lookups to fail.

**Fix:**
```python
from datetime import datetime
episode_date = datetime.now().strftime("%Y-%m-%d-%H%M")
```

---

### ISSUE 5 — MEDIUM: `src/storage.py` — LanceDB string interpolation in WHERE clauses (injection surface)

**File:** `src/storage.py`  
**Lines:** 228, 464, 487, 678, 695  
**Severity:** MEDIUM  

**What breaks:** All WHERE clauses use f-string interpolation: `f"episode_date = '{episode_date}'"`. If an episode_date or story_id contains a single quote, the filter breaks or produces unexpected results. This isn't a traditional SQL injection (LanceDB uses DuckDB SQL under the hood, not a full SQL database), but malformed dates could cause query failures or incorrect matches.

**Current risk:** Low — episode dates are machine-generated (`YYYY-MM-DD-HHMM`). But `make_story_id()` and `make_segment_id()` propagate user-influenced data (HN IDs are external).

**Fix:** Escape single quotes, or use parameterized queries if LanceDB supports them:
```python
safe_date = episode_date.replace("'", "''")
results = table.search().where(f"episode_date = '{safe_date}'", prefilter=True)
```

---

### ISSUE 6 — MEDIUM: `src/storage.py` — `table_names()` deprecated, should be `list_tables()`

**File:** `src/storage.py`  
**Lines:** 102, 110, 118, 535, 716, 722  
**Severity:** MEDIUM  

**What breaks:** CLAUDE.md lesson #15 notes this. `table_names()` still works but emits deprecation warnings on every pipeline run. In a future LanceDB release it may be removed entirely.

**Fix:** Replace all `db.table_names()` with `db.list_tables()`.

---

### ISSUE 7 — MEDIUM: `src/storage.py` — `update_story_script()` creates duplicate rows

**File:** `src/storage.py`  
**Lines:** 408–434  
**Severity:** MEDIUM  

**What breaks:** The docstring acknowledges "LanceDB doesn't support true updates, so this adds a new row." Each script update for a story creates a duplicate row with the same `id`. After a full pipeline run, every story has 2 rows (original + scripted). Functions like `get_story()` use `.limit(1)` which returns whichever row LanceDB finds first — not necessarily the latest.

**Impact:** `get_story()` may return the old row (without script) instead of the updated one. This could cause `update_story_script()` to fail (recursion: gets old row → stores again → gets old row...) or cause `get_stories_by_date()` to return duplicate entries (it uses `.limit(100)` and gets both rows).

**Fix:** Either:
1. Delete old row before adding new one (if LanceDB supports delete by filter)
2. Add a `generated_at` timestamp and make query functions sort by it descending
3. Use merge/upsert if available in your LanceDB version

---

### ISSUE 8 — MEDIUM: `src/pipeline.py` — Double conversion of stories to articles format

**File:** `src/pipeline.py`  
**Lines:** 333–353 and 378–393  
**Severity:** MEDIUM (code quality / correctness risk)  

**What breaks:** When `skip_fetch=False`, the code converts stories to articles format twice — once at line 333 and again at line 378. The second conversion overwrites the first. This is dead code that wastes cycles and creates confusion. More critically, the first conversion (line 333) has slightly different field names than the second (line 378), which could lead to subtle bugs if the code path changes.

**Fix:** Remove the first `articles = [...]` block at lines 333–345. The second block at 378–393 is the one that actually gets used.

---

### ISSUE 9 — MEDIUM: No retry logic in `src/generator.py` for LLM calls

**File:** `src/generator.py`  
**Lines:** 42–53 (`call_claude()`)  
**Severity:** MEDIUM  

**What breaks:** `call_claude()` has no retry logic. If the Claude CLI fails (network blip, rate limit, auth refresh), the entire pipeline crashes. This is called 10 times for scripts + 9 times for interstitials + 1 intro + 1 outro = 21 LLM calls per episode, each a single point of failure.

**Impact:** A single transient failure at story #8 wastes all previous work (stories 1-7 are generated but the pipeline crashes).

**Fix:** Add retry with backoff:
```python
def call_claude(prompt: str, retries: int = 3) -> str:
    for attempt in range(retries):
        try:
            result = subprocess.run(...)
            if result.returncode != 0:
                raise RuntimeError(...)
            return result.stdout.strip()
        except (RuntimeError, subprocess.TimeoutExpired) as e:
            if attempt < retries - 1:
                time.sleep(2 ** attempt)
            else:
                raise
```

---

### ISSUE 10 — MEDIUM: `src/generator.py` — No validation of LLM output

**File:** `src/generator.py`  
**Lines:** 117, 159, 176  
**Severity:** MEDIUM  

**What breaks:** `generate_script()`, `generate_interstitial()`, `generate_intro()`, and `generate_outro()` accept whatever Claude returns. If the LLM returns an error message, an empty string, or markdown-wrapped content, it's treated as a valid script. Intro/outro have `_strip_markdown()` and `_strip_preamble()` hardening, but scripts and interstitials have none.

**Impact:** Garbage LLM output for a script gets stored in LanceDB, written to disk, and sent to TTS. The TTS server happily synthesizes "I'm sorry, I can't help with that" or an empty string.

**Fix:** Add minimum validation:
```python
if not script or count_words(script) < 50:
    raise ValueError(f"Script too short ({count_words(script)} words)")
```

---

### ISSUE 11 — MEDIUM: `scripts/run_episode.sh` — No concurrent run protection

**File:** `scripts/run_episode.sh`  
**Line:** N/A  
**Severity:** MEDIUM  

**What breaks:** If `nohup bash scripts/run_episode.sh` is launched twice (e.g., accidental double-tap, cron overlap), two pipelines run in parallel for the same date. They both fetch the same stories, generate scripts concurrently, and create duplicate LanceDB rows. The TTS step has lock protection (`generate_episode_audio.py`), but the text pipeline (step 1) does not.

**Fix:** Add a PID lockfile at the top of the shell script:
```bash
LOCKFILE="/tmp/dtfhn-${EPISODE_DATE}.lock"
if [ -f "$LOCKFILE" ] && kill -0 "$(cat "$LOCKFILE")" 2>/dev/null; then
    echo "ERROR: Pipeline already running (PID $(cat "$LOCKFILE"))"
    exit 1
fi
echo $$ > "$LOCKFILE"
trap 'rm -f "$LOCKFILE"' EXIT
```

---

### ISSUE 12 — MEDIUM: `scripts/run_episode.sh` — Python inline code uses `sys.argv[1]` without import

**File:** `scripts/run_episode.sh`  
**Line:** 17  
**Severity:** MEDIUM (but works because import is there)  

**What breaks:** Actually, looking more carefully, `sys` is imported via `import sys` and `sys.argv[1]` is used correctly. However, the inline Python block uses shell variable interpolation via `"${EPISODE_DATE}"` passed as a CLI argument to the Python `-c` script — this is the correct fix from the previous audit. **No issue here — false alarm during review.**

*Retracted — keeping for audit trail.*

---

### ISSUE 13 — LOW: `src/storage.py` — LanceDB `store_episode()` allows duplicate episodes

**File:** `src/storage.py`  
**Line:** 192–214  
**Severity:** LOW  

**What breaks:** `store_episode()` always appends (`.add()`). If the pipeline runs twice for the same date, two episode rows exist. `get_episode()` returns whichever `.limit(1)` finds first. The second run's MP3 may or may not be the one returned.

**Impact:** Wasted storage (duplicate ~10MB MP3 binaries) and non-deterministic retrieval.

**Fix:** Check `episode_exists()` before storing, or delete existing row first:
```python
def store_episode(...):
    if episode_exists(episode_date):
        # Delete old row or raise
        raise ValueError(f"Episode {episode_date} already exists")
    ...
```

---

### ISSUE 14 — LOW: `src/hn.py` — `fetch_stories()` returns empty list on total API failure

**File:** `src/hn.py`  
**Line:** 163  
**Severity:** LOW  

**What breaks:** If HN API is down, `fetch_top_story_ids()` returns `[]`, and `fetch_stories()` returns `[]`. The pipeline in `run_episode_pipeline()` checks for this at line 311 (`if not hn_stories: raise RuntimeError`). So the pipeline crashes with a clear error — acceptable behavior.

**Non-issue:** The error path is handled. Noting for completeness.

---

### ISSUE 15 — LOW: `src/scraper.py` — Playwright browser not closed on timeout

**File:** `src/scraper.py`  
**Line:** 243–318  
**Severity:** LOW  

**What breaks:** If `page.goto()` or `page.wait_for_timeout()` throws an exception that isn't caught by the inner try/finally, the browser context leaks. However, the outer `with sync_playwright() as p:` context manager should handle cleanup. The `browser.close()` in the inner finally block is good practice.

**Non-issue:** The context manager pattern handles this correctly. The inner try/finally is belt-and-suspenders.

---

### ISSUE 16 — LOW: `requirements.txt` — Missing `beautifulsoup4` dependency

**File:** `requirements.txt`  
**Severity:** LOW  

**What breaks:** `src/hn.py` imports `from bs4 import BeautifulSoup` but `beautifulsoup4` is not in `requirements.txt`. It works because `newspaper3k` pulls it in as a transitive dependency, but this is fragile — if newspaper3k is replaced or its dependencies change, bs4 silently disappears.

**Fix:** Add to `requirements.txt`:
```
beautifulsoup4>=4.12.0
```

---

### ISSUE 17 — LOW: `scripts/refetch_test.py` — Hardcoded to episode `2026-01-27`

**File:** `scripts/refetch_test.py`  
**Line:** 17  
**Severity:** LOW  

**What breaks:** Like `generate_missing_wavs.py`, this script has a hardcoded episode path. One-off script, but should accept CLI args for reuse.

**Fix:** Same pattern — accept `episode_date` from `sys.argv[1]`.

---

### ISSUE 18 — LOW: `src/pipeline.py` — `run_test_pipeline()` generates non-standard date format

**File:** `src/pipeline.py`  
**Line:** 561  
**Severity:** LOW  

**What breaks:** Test episodes use `f"test-{datetime.now().strftime('%Y%m%d-%H%M%S')}"` which produces dates like `test-20260129-120000`. This doesn't match `YYYY-MM-DD-HHMM` and will cause `format_date_for_tts()` to fail (already handled by try/except at line 451). The test pipeline gracefully falls back to using the raw date string for TTS, which produces garbled spoken output.

**Impact:** Minor — test episodes only. But `format_date_for_tts()` silently degrades.

**Fix:** Use a date format that `format_date_for_tts()` can parse:
```python
episode_date = f"test-{datetime.now().strftime('%Y-%m-%d-%H%M')}"
```
And update `format_date_for_tts()` to strip the `test-` prefix.

---

### ISSUE 19 — LOW: `scripts/generate_episode_audio.py` — `transcript.txt` read may fail

**File:** `scripts/generate_episode_audio.py`  
**Line:** ~375 (`transcript = (episode_dir / "transcript.txt").read_text()`)  
**Severity:** LOW  

**What breaks:** If the text pipeline didn't generate `transcript.txt` (e.g., interrupted), this line raises `FileNotFoundError` after TTS is complete. The MP3 exists but isn't stored in LanceDB.

**Fix:** Fall back to `episode.txt` or construct transcript from segment texts:
```python
transcript_path = episode_dir / "transcript.txt"
if transcript_path.exists():
    transcript = transcript_path.read_text()
else:
    transcript = "\n\n".join(text for _, text in segments)
```

---

### ISSUE 20 — LOW: `src/storage.py` — `get_story()` may return stale data due to duplicate rows

**File:** `src/storage.py`  
**Line:** 457  
**Severity:** LOW (consequence of Issue 7)  

**What breaks:** `get_story()` uses vector search with `.limit(1)`, which means it finds the row closest to some query vector (likely [0,0,...,0] for non-vector searches). With duplicate rows (one with script, one without), there's no guarantee which row is returned.

**Note:** This is a consequence of Issue 7 (duplicate rows from `update_story_script()`). Fixing Issue 7 fixes this.

---

### ISSUE 21 — INFO: `src/tts.py` — `TTS_TIMEOUT` of 3600s (1 hour) per request

**File:** `src/tts.py`  
**Line:** 14  
**Severity:** INFO  

**Note:** Each HTTP request to the TTS server has a 1-hour timeout. Since the server queues requests and may have 21 segments queued across 3 GPUs, this is appropriate. The last request submitted waits for all earlier ones to complete. Not a bug, just documenting the design.

---

### ISSUE 22 — INFO: `src/embeddings.py` — Singleton model never unloaded

**File:** `src/embeddings.py`  
**Line:** 38–46  
**Severity:** INFO  

**Note:** The embedding model (BAAI/bge-large-en-v1.5) is loaded once and held in memory for the lifetime of the process. For the pipeline (runs once, exits), this is fine. For a long-running server, it would need cleanup.

---

### ISSUE 23 — INFO: `src/audio.py` — No error on ffmpeg not found

**File:** `src/audio.py`  
**Lines:** 32, 73, 109, 136  
**Severity:** INFO  

**Note:** All ffmpeg calls use `subprocess.run()` which raises `FileNotFoundError` if ffmpeg isn't installed. The error is unhandled but produces a clear traceback. Could add a pre-flight check for `shutil.which("ffmpeg")` but not critical.

---

## Audit Categories

### 1. Date Format Handling (`YYYY-MM-DD` vs `YYYY-MM-DD-HHMM`)

**Status: MOSTLY FIXED**

- ✅ `src/generator.py:format_date_for_tts()` — Handles both formats (strips `-HHMM`)
- ✅ `src/metadata.py:embed_id3_metadata()` — Handles both formats (strips `-HHMM`)
- ✅ `src/pipeline.py:run_episode_pipeline()` — Default is now `%Y-%m-%d-%H%M`
- ✅ `scripts/run_episode.sh` — Default is now `%Y-%m-%d-%H%M`
- ⚠️ `scripts/scrape_and_load.py` — Still uses `YYYY-MM-DD` (Issue 4)
- ⚠️ `scripts/generate_missing_wavs.py` — Hardcoded to `2026-01-29` (Issue 2)
- ⚠️ `scripts/refetch_test.py` — Hardcoded to `2026-01-27` (Issue 17)
- ✅ All storage functions accept both formats (string comparison, no parsing)
- ✅ Segment ID generation handles both formats (string concatenation)

### 2. Error Handling

| Failure Mode | Handled? | Details |
|---|---|---|
| HN API down | ✅ | 3 retries in `fetch_hn_api()`, pipeline raises RuntimeError on empty |
| All 10 scrapes fail | ✅ | Graceful degradation to "title_only", scripts still generated from titles |
| LLM returns garbage | ⚠️ | No validation (Issue 10). Intro/outro have markdown stripping but no length check |
| LLM call fails | ⚠️ | No retry (Issue 9). Single failure crashes pipeline |
| TTS server unreachable | ✅ | Pre-flight check in `generate_episode_audio.py`, connection error handling in `tts.py` |
| TTS partial failure | ✅ | Robust retry pipeline with backoff, WAV validation, stall detection |
| Disk full | ❌ | No explicit handling. `write_bytes()` / `write_text()` raise OSError. Pipeline crashes. |
| ffmpeg missing | ❌ | FileNotFoundError, unhandled but clear (Issue 23) |

### 3. Shell Script Robustness

- ✅ `set -euo pipefail` — strict mode
- ✅ `cd "$(dirname "$0")/.."` — correct working directory
- ✅ `python3 -u` — unbuffered output for real-time logging
- ✅ Date default: `$(date +%Y-%m-%d-%H%M)`
- ✅ `tee` for dual output (stdout + log file)
- ⚠️ No concurrent run protection (Issue 11)
- ✅ Inline Python uses `sys.argv[1]` not shell interpolation (previous audit fix)
- ✅ No stdin issues — generator uses `stdin=subprocess.DEVNULL`
- ❌ No log rotation (logs accumulate in `/tmp/`)
- ❌ No signal handling (SIGTERM kills mid-pipeline with no cleanup)

### 4. LanceDB Integrity

- ⚠️ **Duplicate rows:** `update_story_script()` creates duplicates (Issue 7)
- ⚠️ **Duplicate episodes:** `store_episode()` allows duplicates (Issue 13)
- ⚠️ **No transactional safety:** Pipeline interruption mid-`store_stories_batch()` leaves partial data
- ✅ **Orphan detection:** Not a concern — stories are keyed by `episode_date`, so orphans from a failed run are self-contained and identifiable
- ❌ **No cleanup mechanism:** Old/failed run data accumulates forever

### 5. TTS Pipeline

- ✅ Lock file prevents concurrent TTS runs
- ✅ Pre-flight queue check (abort/wait/force modes)
- ✅ WAV validation (RIFF header + minimum size)
- ✅ Retry with exponential backoff (3 attempts)
- ✅ Stall detection (5 min with no progress)
- ✅ Race condition recovery (checks for files created after timeout)
- ✅ Proper cleanup (WAVs deleted after MP3 created)
- ⚠️ `generate_missing_wavs.py` bypasses all robustness (Issues 1, 3)

### 6. Generator/Prompt Chain

- ✅ Chained generation passes previous script for variety
- ✅ Dynamic word budgeting with clamp (250-600)
- ✅ Intro/outro have static line enforcement (safety nets)
- ✅ Markdown stripping and preamble removal
- ⚠️ No retry on LLM failure (Issue 9)
- ⚠️ No output validation (Issue 10)
- ✅ Single failure doesn't poison subsequent scripts (each script starts fresh)

### 7. Security

- ✅ No API keys in code (uses `claude` CLI which manages auth)
- ✅ No shell injection — `subprocess.run()` with list args, not `shell=True`
- ⚠️ String interpolation in LanceDB WHERE clauses (Issue 5) — low risk
- ✅ No file path traversal — all paths derived from project root
- ✅ TTS URL is localhost (LAN only)
- ✅ `stdin=subprocess.DEVNULL` prevents interactive hangs

### 8. Dependencies

- ⚠️ `beautifulsoup4` missing from requirements.txt (Issue 16)
- ✅ All other imports have matching requirements
- ✅ `playwright install chromium` noted in requirements comments
- ✅ `lxml_html_clean` included (newspaper3k compatibility)
- ✅ `num2words` included (TTS date formatting)
- ✅ No version conflicts detected

---

## Summary

| Severity | Count | Issues |
|----------|-------|--------|
| CRITICAL | 0 | — |
| HIGH | 3 | #1 (missing em-dash), #2 (hardcoded date), #3 (no WAV validation) |
| MEDIUM | 7 | #4–#11 |
| LOW | 7 | #13–#19 |
| INFO | 3 | #21–#23 |

All HIGH issues are in `scripts/generate_missing_wavs.py` — a one-off utility script. The core pipeline (`src/` + `generate_episode_audio.py` + `run_episode.sh`) is solid.

**Top 3 recommended fixes:**
1. Rewrite `generate_missing_wavs.py` to use `src/tts.py` functions properly (fixes Issues 1, 2, 3)
2. Add retry logic to `call_claude()` in `src/generator.py` (fixes Issue 9)
3. Add LLM output validation in `src/generator.py` (fixes Issue 10)
